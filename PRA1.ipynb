{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping trail running races 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook scraps the trail running races's data from https://itra.run/races. Environment preparation is managed by the following scripts:\n",
    "- WindowsEnvironment.ps1\n",
    "- MacOSEnvironment.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import whois\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "# What environment am I using?\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using selenium, open firefox window with the ITRA website\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://itra.run/races\")\n",
    "\n",
    "# Getting current URL source code \n",
    "get_title = driver.title \n",
    "  \n",
    "# Printing the title of this URL \n",
    "print(get_title) \n",
    "assert \"- ITRA\" in driver.title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Click dropdown menu for language selection\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[1]/nav/div[4]\").click()\n",
    "\n",
    "# Select language EN\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[1]/nav/div[4]/div/div[1]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on PERIOD time selection \"START\"\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div/div/div[1]/div[2]/div/div[2]/div/div[1]/div/div[1]/div[1]/input\").click()\n",
    "\n",
    "# Click on month selection\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div/div/div[1]/div[2]/div/div[2]/div/div[1]/div/div[1]/div[2]/header/span[2]\").click()\n",
    "\n",
    "# Click on January\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div/div/div[1]/div[2]/div/div[2]/div/div[1]/div/div[1]/div[3]/span[1]\").click()\n",
    "\n",
    "# Click on 1st\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div/div/div[1]/div[2]/div/div[2]/div/div[1]/div/div[1]/div[2]/div/span[13]\").click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on PERIOD time selection \"END\"\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div/div/div[1]/div[2]/div/div[2]/div/div[1]/div/div[4]/div[1]/input\").click()\n",
    "\n",
    "# Click on month selection\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div/div/div[1]/div[2]/div/div[2]/div/div[1]/div/div[4]/div[2]/header/span[2]\").click()\n",
    "\n",
    "# Click on year\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div/div/div[1]/div[2]/div/div[2]/div/div[1]/div/div[4]/div[3]/header/span[2]\").click()\n",
    "\n",
    "# Click on 2021\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div/div/div[1]/div[2]/div/div[2]/div/div[1]/div/div[4]/div[4]/span[2]\").click()\n",
    "\n",
    "# Click on December\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div/div/div[1]/div[2]/div/div[2]/div/div[1]/div/div[4]/div[3]/span[12]\").click()\n",
    "\n",
    "\n",
    "# Click on 31st\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div/div/div[1]/div[2]/div/div[2]/div/div[1]/div/div[4]/div[2]/div/span[41]\").click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on More Races to get the full list on the screen & Wait for Visibility of Races\n",
    "# CALDRIA IMPLEMENTAR UN \"CHECK ON WHETHER BUTTON EXISTS\" PER EVITAR L'ERROR...\n",
    "for i in range(43):\n",
    "    driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div/div/div[2]/main/div/button\").click()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting current URL source code \n",
    "get_source = driver.page_source\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping race names with BeautifulSoup\n",
    "soup = BeautifulSoup(get_source, 'html')\n",
    "#print(soup.h5)\n",
    "#soup.find_all('h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the race names\n",
    "racesList = re.findall(r'(?<=<h5 data-v-f3c4ac1c=\"\" class=\"itra-green\">)(.*?)(?=</h5>)', get_source)\n",
    "print(len(racesList))\n",
    "print(racesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mouse over link pointing to the race site\n",
    "links = [a['href'] for a in soup.find_all('a',\"card ontop\", href=True)]\n",
    "print(len(links))\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the data for distance, elevation gain and loss\n",
    "myList = re.findall(r'(?<=<span class=\"icon-text-grey icon-bold\">)(.*?)(?=</span>)', get_source)\n",
    "#print(myList)\n",
    "#len(myList)\n",
    "\n",
    "# Find the race distance\n",
    "distancesList = myList[0::3]\n",
    "print(len(distancesList))\n",
    "\n",
    "# Find the race elevation gain\n",
    "gainList = myList[1::3]\n",
    "print(len(gainList))\n",
    "\n",
    "# Find the race elevation loss\n",
    "lossList = myList[2::3]\n",
    "print(len(lossList))\n",
    "\n",
    "# Find the race date\n",
    "datesList = re.findall(r'(?<=<span data-v-f3c4ac1c=\"\" class=\"itra-grey\" style=\"margin-top: 0.2rem; margin-left: 0.2rem; margin-right: 2rem; font-size: 80%;\">)(.*?)(?=</span>)', get_source)\n",
    "#print(datesList)\n",
    "print(len(datesList))\n",
    "\n",
    "# Loop\n",
    "# Visit race page in Itra\n",
    "# Scraping www, place, topology, number of participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of finishers of the race\n",
    "#<span class=\"icon-finisher icon-bold\">370</span>\n",
    "finishersList = re.findall(r'(?<=<span class=\"icon-finisher icon-bold\">)(.*?)(?=</span>)', get_source)\n",
    "print(finishersList)\n",
    "len(finishersList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign data to tuples: # get the list of tuples from two lists and merge them by using zip(). \n",
    "list_of_tuples = list(zip(racesList, links, distancesList, gainList, lossList, datesList)) \n",
    "# Converting lists of tuples into pandas Dataframe. \n",
    "df = pd.DataFrame(list_of_tuples, columns = ['Name', 'Link', 'Distance', 'Gain', 'Loss', 'Date'])\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAL DESAR LES DADES EN UN FORMAT ADIENT: JSON? CSV directament ja que l'enunciat de la PRACTICA requereix CSV\n",
    "\n",
    "# Convert results to json and save the file\n",
    "with open('result.json', 'w') as fp:\n",
    "    json.dump(sample, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(iList)\n",
    "print(iiList)\n",
    "print(iiList_of_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ara ataquem les dades de la pàgina específica de cada cursa\n",
    "\n",
    "\n",
    "# List with race websites\n",
    "websitesList = []\n",
    "\n",
    "# List with race place and country\n",
    "locationList = []\n",
    "\n",
    "# List containing the table for each website (as a list)\n",
    "tableList = []\n",
    "\n",
    "\n",
    "#slicedLinks = links[:5]\n",
    "\n",
    "for i in tqdm(links):\n",
    "    print(i)\n",
    "    # Using selenium, open firefox window with the ITRA website\n",
    "    driver2 = webdriver.Firefox()\n",
    "    #driver2.get(\"https://itra.run/race/13893\")\n",
    "    driver2.get(i)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        time.sleep(3)   \n",
    "\n",
    "        # Click dropdown menu for language selection\n",
    "        driver2.find_element_by_xpath(\"/html/body/div[1]/div[1]/nav/div[4]\").click()\n",
    "\n",
    "        # Select language EN\n",
    "        driver2.find_element_by_xpath(\"/html/body/div[1]/div[1]/nav/div[4]/div/div[1]\").click()\n",
    "\n",
    "        time.sleep(15)\n",
    "        \n",
    "        # if link is broken, go back\n",
    "    except TimeoutException:\n",
    "        driver2.back()\n",
    "        print(\"Time out exception.\")\n",
    "        # continue so we can return to beginning of loop\n",
    "        continue\n",
    "\n",
    "    # if you reach this point, the link is valid, and you can 'do stuff' on the page\n",
    "    \n",
    "    # Getting current URL source code \n",
    "    get_source2 = driver2.page_source\n",
    "    time.sleep(2)   \n",
    "    driver2.close()\n",
    "\n",
    "    # Scraping race names with BeautifulSoup\n",
    "    soup2 = BeautifulSoup(get_source2, 'html')\n",
    "\n",
    "    # List containing the race website (when available) and \"facebook\", \"twitter\" and other info\n",
    "    hrefList = [a['href'] for a in soup2.find_all('a', {'rel': \"ugc\"}, href=True)]\n",
    "\n",
    "    # Remove the links that contain \"facebook\" or \"twitter\" or \"@\"\n",
    "    hrefList[:] = [x for x in hrefList if \"facebook\" not in x]\n",
    "    hrefList[:] = [x for x in hrefList if \"twitter\" not in x]\n",
    "    hrefList[:] = [x for x in hrefList if \"@\" not in x]\n",
    "\n",
    "    print(len(hrefList))\n",
    "    print(hrefList) \n",
    "    websitesList.append(hrefList)\n",
    "    \n",
    "    # Srape the table with additional data: data labels (first) and content\n",
    "    labelsList  = re.findall(r'(?<=<div class=\"colinforace1\">)(.*?)(?=</div>)', get_source2)\n",
    "    contentList = re.findall(r'(?<=<div class=\"colinforace2 mbb\">)(.*?)(?=</div>)', get_source2)\n",
    "\n",
    "    labels_content_list_of_tuples = list(zip(labelsList[:13], contentList[:13])) \n",
    "    print(labels_content_list_of_tuples)\n",
    "    tableList.append(labels_content_list_of_tuples)\n",
    "    \n",
    "    # Scrape the location\n",
    "    location = soup2.find('p').getText()\n",
    "    print(location)\n",
    "    locationList.append(location)\n",
    "    \n",
    "    # Check scraped data is saved in a manner that can be directly matched\n",
    "    print(len(websitesList))\n",
    "    print(len(locationList))\n",
    "    print(len(tableList))\n",
    "    \n",
    "print(websitesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('TestI001': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ff77ec2af973d4b53a733ca12490701ff886b2d76499657187269f78fd80e8b3"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}